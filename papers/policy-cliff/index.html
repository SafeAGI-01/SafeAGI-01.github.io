<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="description" content="The Policy Cliff: A Theoretical Analysis of Reward-Policy Maps in Large Language Models">
  <meta name="keywords" content="LLM, RLHF, Policy Stability, AI Safety, Reinforcement Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>The Policy Cliff | SafeAGI</title>

  <!-- Open Graph -->
  <meta property="og:type" content="article">
  <meta property="og:title" content="The Policy Cliff: A Theoretical Analysis of Reward-Policy Maps in Large Language Models">
  <meta property="og:description" content="A rigorous mathematical framework analyzing why RL often produces brittle policies in LLMs.">
  <meta property="og:url" content="https://safeagi-01.github.io/papers/policy-cliff/">

  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans:400,500,700|Noto+Sans:400,500,700&display=swap" rel="stylesheet">

  <!-- Stylesheets -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="../../static/css/style.css?v=3">
  <!-- Inline dark mode overrides â€” guarantees Bulma is beaten -->
  <style>
    html[data-theme="dark"] body{background:#18181b!important;color:#e4e4e7!important}
    html[data-theme="dark"] .section,html[data-theme="dark"] .hero-body{background-color:transparent!important}
    html[data-theme="dark"] .navbar,html[data-theme="dark"] .navbar.is-fixed-top{background:rgba(24,24,27,.88)!important}
    html[data-theme="dark"] .navbar-menu{background-color:rgba(24,24,27,.96)!important}
    html[data-theme="dark"] .navbar-item{color:#d4d4d8!important}
    html[data-theme="dark"] .box,html[data-theme="dark"] .content-card{background:#27272a!important;color:#d4d4d8!important;border-color:rgba(255,255,255,.08)!important}
    html[data-theme="dark"] .title,html[data-theme="dark"] .subtitle{color:#e4e4e7!important}
    html[data-theme="dark"] .content p,html[data-theme="dark"] .content li{color:#d4d4d8!important}
    html[data-theme="dark"] .has-text-grey{color:#a1a1aa!important}
    html[data-theme="dark"] strong{color:#e4e4e7!important}
    html[data-theme="dark"] .footer{background:linear-gradient(135deg,#09090b,#18181b)!important}
  </style>
  <script>
    (function(){var t=localStorage.getItem('safeagi-theme');if(t){document.documentElement.setAttribute('data-theme',t)}else{document.documentElement.setAttribute('data-theme',window.matchMedia('(prefers-color-scheme:dark)').matches?'dark':'light')}})();
  </script>
</head>
<body>

<!-- Page Loading -->
<div class="page-loading">
  <div class="loader"></div>
</div>

<!-- Fixed Navbar -->
<nav class="navbar is-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-brand">
      <a class="navbar-item" href="../../">
        <strong>SafeAGI</strong>
      </a>
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false" data-target="navMenu">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div id="navMenu" class="navbar-menu">
      <div class="navbar-start">
        <a class="navbar-item" href="../../">Home</a>
        <a class="navbar-item" href="../">Papers</a>
      </div>
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/SafeAGI-01" target="_blank">
          <span class="icon"><i class="fab fa-github"></i></span>
        </a>
        <div class="navbar-item">
          <button class="theme-toggle" id="themeToggle" title="Toggle dark mode" aria-label="Toggle dark mode">
            <svg class="icon-sun" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
            <svg class="icon-moon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/></svg>
          </button>
        </div>
      </div>
    </div>
  </div>
</nav>

<!-- Hero Section -->
<section class="hero hero-gradient hero-animated" style="padding-top: 52px;">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title has-text-white">The Policy Cliff: A Theoretical Analysis of Reward-Policy Maps in Large Language Models</h1>

          <div class="is-size-5 publication-authors mt-4 has-text-white-ter">
            <span class="author-block">
              <a href="mailto:xingcheng.xu18@gmail.com" class="has-text-white">Xingcheng Xu</a>
            </span>
          </div>

          <div class="is-size-6 mt-2 has-text-white-ter">
            <span>Shanghai Artificial Intelligence Laboratory</span>
          </div>

          <div class="mt-4">
            <span class="tag is-medium is-light">2025-07-27</span>
            <span class="tag is-medium is-warning">arXiv:2507.20150</span>
          </div>

          <div class="publication-links mt-5">
            <a href="The-Policy-Cliff-Paper-2025.pdf" class="button is-white is-outlined is-medium" target="_blank">
              <span class="icon"><i class="fas fa-file-pdf"></i></span>
              <span>Paper</span>
            </a>
            <a href="https://arxiv.org/abs/2507.20150" class="button is-white is-outlined is-medium" target="_blank">
              <span class="icon"><i class="ai ai-arxiv"></i></span>
              <span>arXiv</span>
            </a>
            <a href="https://github.com/SafeAGI-01/PolicyCliff" class="button is-white is-medium" target="_blank">
              <span class="icon"><i class="fab fa-github"></i></span>
              <span>TeX Source</span>
            </a>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Abstract -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 section-title">Abstract</h2>
        <div class="content has-text-justified">
          <p class="drop-cap">
            Reinforcement learning (RL) plays a crucial role in shaping the behavior of large language and reasoning models (LLMs/LRMs). However, it often produces brittle and unstable policies, leading to critical failures such as spurious reasoning, deceptive alignment, and instruction disobedience that undermine the trustworthiness and safety of LLMs/LRMs. Currently, these issues lack a unified theoretical explanation and are typically addressed using ad-hoc heuristics. This paper presents a rigorous mathematical framework for analyzing the stability of the mapping from a reward function to the optimal policy. We show that policy brittleness often stems from non-unique optimal actions, a common occurrence when multiple valid traces exist in a reasoning task. This theoretical lens provides a unified explanation for a range of seemingly disparate failures, reframing them as rational outcomes of optimizing rewards that may be incomplete or noisy, especially in the presence of action degeneracy. We extend this analysis from the fundamental single-reward setting to the more realistic multi-reward RL across diverse domains, showing how stability is governed by an "effective reward" aggregation mechanism. We also prove that entropy regularization restores policy stability at the cost of increased stochasticity. Our framework provides a unified explanation for recent empirical findings on deceptive reasoning, instruction-following trade-offs, and RLHF-induced sophistry, and is further validated through perturbation experiments in multi-reward RL. This work advances policy-stability analysis from empirical heuristics towards a principled theory, offering essential insights for designing safer and more trustworthy AI systems.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Overview -->
<section class="section section-alt">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 section-title">Overview</h2>
        <div class="content has-text-justified">
          <p>
            This paper develops a rigorous mathematical framework to analyze why reinforcement learning often produces brittle and unstable policies in large language models. The core contribution is proving that the reward-policy map is continuous when optimal actions are unique, but becomes discontinuous (exhibiting "policy cliffs") when multiple optimal actions exist. This theoretical insight provides a unified explanation for various LLM alignment failures including spurious reasoning, deceptive alignment, instruction disobedience, and RLHF-induced sophistry. The framework extends to multi-reward RL settings and demonstrates that entropy regularization can restore policy stability. The work bridges theoretical analysis with empirical observations, offering principled guidance for designing safer AI systems.
          </p>
        </div>

        <h3 class="title is-4 section-title mt-5">Key Contributions</h3>
        <div class="content">
          <ul>
            <li><strong>Theoretical Framework:</strong> Formal analysis of the reward-policy map showing policy instability stems from non-unique optimal actions</li>
            <li><strong>Unified Explanation:</strong> Mathematical lens connecting spurious reasoning, deceptive alignment, instruction-following failures, and RLHF sophistry</li>
            <li><strong>Multi-Reward Analysis:</strong> Extension to realistic multi-reward RL with "effective reward" aggregation mechanism</li>
            <li><strong>Entropy Regularization:</strong> Proof that entropy regularization restores Lipschitz continuity to the reward-policy map</li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- BibTeX -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 section-title">BibTeX</h2>
        <div class="bibtex-container">
          <button class="copy-button">
            <i class="fas fa-copy"></i> Copy
          </button>
          <pre><code>@article{xu2025policy,
  title={The policy cliff: A theoretical analysis of reward-policy maps in large language models},
  author={Xu, Xingcheng},
  journal={arXiv preprint arXiv:2507.20150},
  year={2025}
}</code></pre>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Footer -->
<footer class="footer">
  <div class="content has-text-centered">
    <p class="mb-3">
      <a class="mr-4" href="https://github.com/SafeAGI-01" target="_blank">
        <i class="fab fa-github fa-2x"></i>
      </a>
    </p>
    <p class="is-size-7 has-text-grey">
      SafeAGI Research Group<br>
      Shanghai Artificial Intelligence Laboratory
    </p>
  </div>
</footer>

<!-- Back to Top Button -->
<button class="back-to-top" aria-label="Back to top">
  <i class="fas fa-arrow-up"></i>
</button>

<!-- JavaScript -->
<script src="../../static/js/main.js"></script>

</body>
</html>
