<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="description" content="Epistemic Traps: Rational Misalignment Driven by Model Misspecification">
  <meta name="keywords" content="LLM, AI Safety, Misalignment, Sycophancy, Hallucination, Deception, Berk-Nash">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Epistemic Traps | SafeAGI</title>

  <!-- Open Graph -->
  <meta property="og:type" content="article">
  <meta property="og:title" content="Epistemic Traps: Rational Misalignment Driven by Model Misspecification">
  <meta property="og:description" content="AI misalignment behaviors are mathematically rationalizable outcomes of model misspecification.">
  <meta property="og:url" content="https://safeagi-01.github.io/papers/rational-misalignment/">

  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans:400,500,700|Noto+Sans:400,500,700&display=swap" rel="stylesheet">

  <!-- Stylesheets -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="../../static/css/style.css">
  <script>
    (function(){var t=localStorage.getItem('safeagi-theme');if(t){document.documentElement.setAttribute('data-theme',t)}else{document.documentElement.setAttribute('data-theme',window.matchMedia('(prefers-color-scheme:dark)').matches?'dark':'light')}})();
  </script>
</head>
<body>

<!-- Page Loading -->
<div class="page-loading">
  <div class="loader"></div>
</div>

<!-- Fixed Navbar -->
<nav class="navbar is-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-brand">
      <a class="navbar-item" href="../../">
        <strong>SafeAGI</strong>
      </a>
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false" data-target="navMenu">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div id="navMenu" class="navbar-menu">
      <div class="navbar-start">
        <a class="navbar-item" href="../../">Home</a>
        <a class="navbar-item" href="../">Papers</a>
      </div>
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/SafeAGI-01" target="_blank">
          <span class="icon"><i class="fab fa-github"></i></span>
        </a>
        <div class="navbar-item">
          <button class="theme-toggle" id="themeToggle" title="Toggle dark mode" aria-label="Toggle dark mode">
            <svg class="icon-sun" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
            <svg class="icon-moon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/></svg>
          </button>
        </div>
      </div>
    </div>
  </div>
</nav>

<!-- Hero Section -->
<section class="hero hero-gradient hero-animated" style="padding-top: 52px;">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title has-text-white">Epistemic Traps: Rational Misalignment Driven by Model Misspecification</h1>

          <div class="is-size-5 publication-authors mt-4 has-text-white-ter">
            <span class="author-block"><a href="mailto:xuxingcheng@pjlab.org.cn" class="has-text-white">Xingcheng Xu</a><sup>1</sup>,</span>
            <span class="author-block">Jingjing Qu<sup>1</sup>,</span>
            <span class="author-block">Qiaosheng Zhang<sup>1</sup>,</span>
            <span class="author-block">Chaochao Lu<sup>1</sup>,</span><br>
            <span class="author-block">Yanqing Yang<sup>2</sup>,</span>
            <span class="author-block">Na Zou<sup>1</sup>,</span>
            <span class="author-block">Xia Hu<sup>1</sup></span>
          </div>

          <div class="is-size-6 mt-2 has-text-white-ter">
            <span><sup>1</sup>Shanghai Artificial Intelligence Laboratory,</span>
            <span><sup>2</sup>ShanghaiTech University</span>
          </div>

          <div class="mt-4">
            <span class="tag is-medium is-light">2026-01-27</span>
            <span class="tag is-medium is-warning">arXiv:2602.17676</span>
          </div>

          <div class="publication-links mt-5">
            <a href="Rational-Misalignment-Paper-2026-01-27.pdf" class="button is-white is-outlined is-medium" target="_blank">
              <span class="icon"><i class="fas fa-file-pdf"></i></span>
              <span>Paper</span>
            </a>
            <a href="https://arxiv.org/abs/2602.17676" class="button is-white is-outlined is-medium" target="_blank">
              <span class="icon"><i class="ai ai-arxiv"></i></span>
              <span>arXiv</span>
            </a>
            <a href="https://github.com/SafeAGI-01/RationalMisalignment" class="button is-white is-medium" target="_blank">
              <span class="icon"><i class="fab fa-github"></i></span>
              <span>TeX Source</span>
            </a>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Quote -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <div class="quote-block">
          <p class="is-size-5">"Safety is an internal property of the agent's priors, not just an external property of the environment."</p>
          <p class="has-text-right mt-2 has-text-grey">â€” The Authors</p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Abstract -->
<section class="section" style="padding-top: 0;">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 section-title">Abstract</h2>
        <div class="content has-text-justified">
          <p class="drop-cap">
            The rapid deployment of Large Language Models and AI agents across critical societal and technical domains is hindered by persistent behavioral pathologies including sycophancy, hallucination, and strategic deception that resist mitigation via reinforcement learning. Current safety paradigms treat these failures as transient training artifacts, lacking a unified theoretical framework to explain their emergence and stability. Here we show that these misalignments are not errors, but mathematically rationalizable behaviors arising from model misspecification. By adapting Berk-Nash Rationalizability from theoretical economics to artificial intelligence, we derive a rigorous framework that models the agent as optimizing against a flawed subjective world model. We demonstrate that widely observed failures are structural necessities: unsafe behaviors emerge as either a stable misaligned equilibrium or oscillatory cycles depending on reward scheme, while strategic deception persists as a "locked-in" equilibrium or through epistemic indeterminacy robust to objective risks. We validate these theoretical predictions through behavioral experiments on six state-of-the-art model families, generating phase diagrams that precisely map the topological boundaries of safe behavior. Our findings reveal that safety is a discrete phase determined by the agent's epistemic priors rather than a continuous function of reward magnitude. This establishes Subjective Model Engineering, defined as the design of an agent's internal belief structure, as a necessary condition for robust alignment, marking a paradigm shift from manipulating environmental rewards to shaping the agent's interpretation of reality.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Overview -->
<section class="section section-alt">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 section-title">Overview</h2>
        <div class="content has-text-justified">
          <p>
            This paper demonstrates that persistent AI misalignment behaviors such as sycophancy, hallucination, and strategic deception are not training errors but mathematically rationalizable outcomes arising from model misspecification. By adapting Berk-Nash Rationalizability from theoretical economics to AI, the authors show that unsafe behaviors emerge as stable equilibria or oscillatory cycles depending on reward schemes. The framework is validated through extensive behavioral experiments on six state-of-the-art model families (including GPT-4o, GPT-5, Qwen, DeepSeek, and Gemini) with over 30,000 trials. The key insight is that safety is a discrete topological phase determined by the agent's epistemic priors rather than a continuous function of reward magnitude, proposing a paradigm shift from reward engineering to "Subjective Model Engineering."
          </p>
        </div>

        <h3 class="title is-4 section-title mt-5">Key Findings</h3>
        <div class="content">
          <ul>
            <li><strong>Alignment failures are structurally stable equilibria</strong> driven by internal model misspecification rather than transient errors</li>
            <li><strong>Epistemic misspecification invalidates standard Nash Equilibrium</strong> safety guarantees for AI agents</li>
            <li><strong>Phase space analysis reveals critical transitions</strong> where safety collapses into stable misalignment or oscillatory behaviors</li>
            <li><strong>Strategic deception is governed by belief topology</strong> rather than the magnitude of objective penalties</li>
            <li><strong>Safety requires constraining subjective reality</strong> to render unsafe behaviors mathematically non-rationalizable</li>
          </ul>
        </div>

        <h3 class="title is-4 section-title mt-5">Experimental Validation</h3>
        <div class="content">
          <ul>
            <li><strong>Scale:</strong> 30,000+ independent trials, 1.5+ million agent-environment interactions</li>
            <li><strong>Models Tested:</strong> Qwen2.5-72B, Qwen3-235B, DeepSeek-V3.2 (685B), Gemini-2.5 Flash, GPT-4o mini, GPT-5 Nano</li>
            <li><strong>Experiments:</strong> Sycophancy phase diagrams, strategic deception under epistemic constraints</li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- BibTeX -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 section-title">BibTeX</h2>
        <div class="bibtex-container">
          <button class="copy-button">
            <i class="fas fa-copy"></i> Copy
          </button>
          <pre><code>@article{xu2026epistemic,
  title={Epistemic Traps: Rational Misalignment Driven by Model Misspecification},
  author={Xu, Xingcheng and Qu, Jingjing and Zhang, Qiaosheng and Lu, Chaochao and Yang, Yanqing and Zou, Na and Hu, Xia},
  journal={arXiv preprint arXiv:2602.17676},
  year={2026}
}</code></pre>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Footer -->
<footer class="footer">
  <div class="content has-text-centered">
    <p class="mb-3">
      <a class="mr-4" href="https://github.com/SafeAGI-01" target="_blank">
        <i class="fab fa-github fa-2x"></i>
      </a>
    </p>
    <p class="is-size-7 has-text-grey">
      SafeAGI Research Group<br>
      Shanghai Artificial Intelligence Laboratory
    </p>
  </div>
</footer>

<!-- Back to Top Button -->
<button class="back-to-top" aria-label="Back to top">
  <i class="fas fa-arrow-up"></i>
</button>

<!-- JavaScript -->
<script src="../../static/js/main.js"></script>

</body>
</html>
