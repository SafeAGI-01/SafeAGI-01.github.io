<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="description" content="SafeAGI - Research on AI Safety and Alignment">
  <meta name="keywords" content="AI Safety, Alignment, LLM, Large Language Models, Machine Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="author" content="SafeAGI Research Group">
  <title>SafeAGI | AI Safety Research</title>

  <!-- Open Graph / Social Media -->
  <meta property="og:type" content="website">
  <meta property="og:title" content="SafeAGI | AI Safety Research">
  <meta property="og:description" content="Research on AI Safety and Alignment - Understanding and improving the safety of large language models and AI agents.">
  <meta property="og:url" content="https://safeagi-01.github.io/">
  <meta property="twitter:card" content="summary_large_image">

  <!-- Preload Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preload" href="https://fonts.googleapis.com/css?family=Google+Sans:400,500,700|Noto+Sans:400,500,700&display=swap" as="style">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans:400,500,700|Noto+Sans:400,500,700&display=swap" rel="stylesheet">

  <!-- Stylesheets -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/style.css">
</head>
<body>

<!-- Page Loading -->
<div class="page-loading">
  <div class="loader"></div>
</div>

<!-- Fixed Navbar -->
<nav class="navbar is-fixed-top is-light" role="navigation" aria-label="main navigation">
  <div class="container">
    <div class="navbar-brand">
      <a class="navbar-item" href="./">
        <strong style="color: var(--bg-gradient-start);">SafeAGI</strong>
      </a>
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false" data-target="navMenu">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div id="navMenu" class="navbar-menu">
      <div class="navbar-start">
        <a class="navbar-item is-active" href="./">Home</a>
        <a class="navbar-item" href="papers/">Papers</a>
      </div>
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/SafeAGI-01" target="_blank">
          <span class="icon"><i class="fab fa-github"></i></span>
        </a>
      </div>
    </div>
  </div>
</nav>

<!-- Hero Section -->
<section class="hero is-medium hero-gradient hero-animated" style="padding-top: 52px;">
  <div class="hero-body">
    <div class="container has-text-centered">
      <h1 class="title is-1 hero-title has-text-white">SafeAGI</h1>
      <h2 class="subtitle is-4 has-text-white-ter">Research on AI Safety and Alignment</h2>
      <div class="buttons is-centered mt-5">
        <a href="papers/" class="button is-white is-outlined is-medium">
          <span class="icon"><i class="fas fa-file-alt"></i></span>
          <span>View Papers</span>
        </a>
        <a href="https://github.com/SafeAGI-01" class="button is-white is-medium" target="_blank">
          <span class="icon"><i class="fab fa-github"></i></span>
          <span>GitHub</span>
        </a>
      </div>
    </div>
  </div>
</section>

<!-- About Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-centered section-title">About</h2>
        <div class="content has-text-justified">
          <p class="drop-cap">
            SafeAGI is a research initiative focused on understanding and improving the safety and alignment of artificial intelligence systems, particularly large language models (LLMs) and AI agents. Our work combines rigorous theoretical analysis with empirical validation to address critical challenges in AI safety, including policy stability, model misspecification, and alignment failures.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Papers Section -->
<section class="section section-alt">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered section-title mb-6">Papers</h2>
    <div class="columns is-centered">
      <div class="column is-10">

        <!-- Paper 1: Policy Cliff -->
        <div class="box paper-card mb-5" style="position: relative;">
          <span class="badge-new">New</span>
          <article class="media">
            <figure class="media-left is-hidden-mobile">
              <div class="paper-thumbnail">
                <i class="fas fa-chart-line"></i>
              </div>
            </figure>
            <div class="media-content">
              <div class="content">
                <h3 class="title is-4 publication-title mb-2">
                  <a href="papers/policy-cliff/" style="color: var(--text-dark);">The Policy Cliff: A Theoretical Analysis of Reward-Policy Maps in Large Language Models</a>
                </h3>
                <p class="is-size-6 has-text-grey mb-2">
                  Xingcheng Xu
                </p>
                <p class="is-size-7 mb-3">
                  <span class="tag is-info is-light">2025</span>
                  <span class="tag is-warning is-light">arXiv:2507.20150</span>
                  <span class="tag is-link is-light">Shanghai AI Lab</span>
                </p>
                <details class="mb-2">
                  <summary class="is-size-6 has-text-weight-semibold" style="cursor: pointer;">
                    <i class="fas fa-chevron-right mr-1"></i> Abstract
                  </summary>
                  <p class="is-size-6 mt-3 has-text-justified" style="padding-left: 1rem; border-left: 3px solid var(--bg-gradient-start);">
                    Reinforcement learning (RL) plays a crucial role in shaping the behavior of large language and reasoning models (LLMs/LRMs). However, it often produces brittle and unstable policies, leading to critical failures such as spurious reasoning, deceptive alignment, and instruction disobedience that undermine the trustworthiness and safety of LLMs/LRMs. Currently, these issues lack a unified theoretical explanation and are typically addressed using ad-hoc heuristics. This paper presents a rigorous mathematical framework for analyzing the stability of the mapping from a reward function to the optimal policy. We show that policy brittleness often stems from non-unique optimal actions, a common occurrence when multiple valid traces exist in a reasoning task. This theoretical lens provides a unified explanation for a range of seemingly disparate failures, reframing them as rational outcomes of optimizing rewards that may be incomplete or noisy, especially in the presence of action degeneracy. We extend this analysis from the fundamental single-reward setting to the more realistic multi-reward RL across diverse domains, showing how stability is governed by an "effective reward" aggregation mechanism. We also prove that entropy regularization restores policy stability at the cost of increased stochasticity. Our framework provides a unified explanation for recent empirical findings on deceptive reasoning, instruction-following trade-offs, and RLHF-induced sophistry, and is further validated through perturbation experiments in multi-reward RL. This work advances policy-stability analysis from empirical heuristics towards a principled theory, offering essential insights for designing safer and more trustworthy AI systems.
                  </p>
                </details>
              </div>
              <nav class="level is-mobile mt-4">
                <div class="level-left">
                  <a class="button is-small is-rounded is-dark" href="papers/policy-cliff/">
                    <span class="icon"><i class="fas fa-globe"></i></span>
                    <span>Project</span>
                  </a>
                  <a class="button is-small is-rounded is-dark ml-2" href="papers/policy-cliff/The-Policy-Cliff-Paper-2025.pdf">
                    <span class="icon"><i class="fas fa-file-pdf"></i></span>
                    <span>PDF</span>
                  </a>
                  <a class="button is-small is-rounded is-dark ml-2" href="https://arxiv.org/abs/2507.20150">
                    <span class="icon"><i class="ai ai-arxiv"></i></span>
                    <span>arXiv</span>
                  </a>
                  <a class="button is-small is-rounded is-dark ml-2" href="https://github.com/SafeAGI-01/PolicyCliff">
                    <span class="icon"><i class="fab fa-github"></i></span>
                    <span>TeX</span>
                  </a>
                </div>
              </nav>
            </div>
          </article>
        </div>

        <!-- Paper 2: Rational Misalignment -->
        <div class="box paper-card" style="position: relative;">
          <span class="badge-new">New</span>
          <article class="media">
            <figure class="media-left is-hidden-mobile">
              <div class="paper-thumbnail">
                <i class="fas fa-brain"></i>
              </div>
            </figure>
            <div class="media-content">
              <div class="content">
                <h3 class="title is-4 publication-title mb-2">
                  <a href="papers/rational-misalignment/" style="color: var(--text-dark);">Epistemic Traps: Rational Misalignment Driven by Model Misspecification</a>
                </h3>
                <p class="is-size-6 has-text-grey mb-2">
                  Xingcheng Xu, Jingjing Qu, Qiaosheng Zhang, Chaochao Lu, Yanqing Yang, Na Zou, Xia Hu
                </p>
                <p class="is-size-7 mb-3">
                  <span class="tag is-info is-light">2026</span>
                  <span class="tag is-warning is-light">arXiv:2602.17676</span>
                  <span class="tag is-link is-light">Shanghai AI Lab & ShanghaiTech</span>
                </p>
                <details class="mb-2">
                  <summary class="is-size-6 has-text-weight-semibold" style="cursor: pointer;">
                    <i class="fas fa-chevron-right mr-1"></i> Abstract
                  </summary>
                  <p class="is-size-6 mt-3 has-text-justified" style="padding-left: 1rem; border-left: 3px solid var(--bg-gradient-start);">
                    The rapid deployment of Large Language Models and AI agents across critical societal and technical domains is hindered by persistent behavioral pathologies including sycophancy, hallucination, and strategic deception that resist mitigation via reinforcement learning. Current safety paradigms treat these failures as transient training artifacts, lacking a unified theoretical framework to explain their emergence and stability. Here we show that these misalignments are not errors, but mathematically rationalizable behaviors arising from model misspecification. By adapting Berk-Nash Rationalizability from theoretical economics to artificial intelligence, we derive a rigorous framework that models the agent as optimizing against a flawed subjective world model. We demonstrate that widely observed failures are structural necessities: unsafe behaviors emerge as either a stable misaligned equilibrium or oscillatory cycles depending on reward scheme, while strategic deception persists as a "locked-in" equilibrium or through epistemic indeterminacy robust to objective risks. We validate these theoretical predictions through behavioral experiments on six state-of-the-art model families, generating phase diagrams that precisely map the topological boundaries of safe behavior. Our findings reveal that safety is a discrete phase determined by the agent's epistemic priors rather than a continuous function of reward magnitude. This establishes Subjective Model Engineering, defined as the design of an agent's internal belief structure, as a necessary condition for robust alignment, marking a paradigm shift from manipulating environmental rewards to shaping the agent's interpretation of reality.
                  </p>
                </details>
              </div>
              <nav class="level is-mobile mt-4">
                <div class="level-left">
                  <a class="button is-small is-rounded is-dark" href="papers/rational-misalignment/">
                    <span class="icon"><i class="fas fa-globe"></i></span>
                    <span>Project</span>
                  </a>
                  <a class="button is-small is-rounded is-dark ml-2" href="papers/rational-misalignment/Rational-Misalignment-Paper-2026-01-27.pdf">
                    <span class="icon"><i class="fas fa-file-pdf"></i></span>
                    <span>PDF</span>
                  </a>
                  <a class="button is-small is-rounded is-dark ml-2" href="https://arxiv.org/abs/2602.17676">
                    <span class="icon"><i class="ai ai-arxiv"></i></span>
                    <span>arXiv</span>
                  </a>
                  <a class="button is-small is-rounded is-dark ml-2" href="https://github.com/SafeAGI-01/RationalMisalignment">
                    <span class="icon"><i class="fab fa-github"></i></span>
                    <span>TeX</span>
                  </a>
                </div>
              </nav>
            </div>
          </article>
        </div>

      </div>
    </div>
  </div>
</section>

<!-- Footer -->
<footer class="footer">
  <div class="content has-text-centered">
    <p class="mb-3">
      <a class="mr-4" href="https://github.com/SafeAGI-01" target="_blank">
        <i class="fab fa-github fa-2x"></i>
      </a>
    </p>
    <p class="is-size-7 has-text-grey">
      SafeAGI Research Group<br>
      Shanghai Artificial Intelligence Laboratory
    </p>
  </div>
</footer>

<!-- Back to Top Button -->
<button class="back-to-top" aria-label="Back to top">
  <i class="fas fa-arrow-up"></i>
</button>

<!-- JavaScript -->
<script src="static/js/main.js"></script>

</body>
</html>
